# Dimensions of the final video
width: 512
height: 512
# Frame rate of the final video
frames_per_second: 18

# Path for output frames and other files
output_path: generated/

# A directory for intermediary results, and misc. other models that are required
persistence_dir: persistence/

# Torch device to use
torch_device: cuda

scenes:
  - # The prompt to generate images from
    prompt: asd
    # How long this prompt should last at the frame rate specified above
    duration: 46s
    # Interpolation between the previous and this scene. does not add to the duration of the scene. The initial frames of this scene will contain the interpolated frames
    interpolation: 0s
    # Which mechanism to use to generate frames. Must be defined with parameters and type below
    mechanism: default
    # Optionally, some parameters of a mechanism may be modified at the scene-level. For example the number of steps to sample for in a diffusion model.
    mechanism_parameters:
      steps: 50

mechanisms:
  - # Name of this parameter set within the config. Can be referenced in scenes
    name: default
    # Name of the mechanism to use
    type: turbo-stablediff
    # Arbitrary parameters the text2video mechanism can take. These are always implementation specific
    mechanism_parameters:
      # Model to use
      model_path: /path/to/model
      # Config for the model to use
      model_config_path: /path/to/config.yaml
      # Sampling steps per frame
      steps: 20
      # Type of sampler
      sampler: klms
      # Unconditional guidance scale
      scale: 7

      # How many turbo steps (lower amount of sampling steps) to run in-between "full sized" frames
      turbo_steps: 1
      # How many sampling steps to run for a turbo frame
      turbo_sampling_steps: 10
      # Noise schedule (how much noise to add to in-between frames to improve variance), can be a function or static
      noise_schedule: 0.02
      strength_schedule: 0.65
      contrast_schedule: 1.0
      ddim_eta: 0.0
      dynamic_threshold:
      static_threshold:
      half_precision: true

      use_init: false
      init_image: ""
      init_strength: 0.0
      init_use_mask: false
      strength: 0
      use_mask: false
      init_c: null

      seed: -1


      # Type of animation to apply between frames, if any
      animation: 3D
      # Parameters for the animation
      # parameters handled in Animator3D class
      animation_parameters:
        translation_x: 0
        translation_y: 0
        translation_z: 1
        # depth model parameters
        near_plane: 20
        far_plane: 10000
        fov: 40
        sampling_mode: bicubic
        padding_mode: border
  - # Name of this parameter set within the config. Can be referenced in scenes
    name: api
    # Name of the mechanism to use
    type: api
    # Arbitrary parameters the text2video mechanism can take. These are always implementation specific
    mechanism_parameters:
      host: http://localhost:7860
      seed: 123456
      # Additional parameters to format the txt2img request body template with (prompt is auto-filled)
      txt2img_params:
        steps: 20
        # Euler a, LMS, ...
        sampler: LMS
        scale: 7.0
        W: 768
        H: 512
      img2img_params:
        steps: 20
        # Euler a, LMS, ...
        sampler: LMS
        scale: 7.0
        W: 768
        H: 512
        # This is denoising strength here, lower = better flow
        strength: 0.45
      # Type of animation to apply between frames, if any
      animation: 3D
      # Parameters for the animation
      animation_parameters:
        translation_x: 0
        translation_y: beat*2*1*sin(1/4*t)
        translation_z: 0.5+beat*7
        # depth model parameters
        near_plane: 200
        far_plane: 10000
        fov: 40
        sampling_mode: bicubic
        padding_mode: border
# Other, global multi-modal context
additional_context:
  audio_reactivity:
    # Audio reactivity configuration
    # The type of audio-reactivity to perform.
    #   spectral is a very simple type of audio-reactivity that uses bandpass filters and returns a normalized 0..1 range value for each filter's amplitude at each given point in time.
    #   beat-librosa uses librosa's beat detection mechanism for detecting beats and simply returns a "beat" variable that is either 0 or 1 in the function environment.
    type: "spectral"
    # Path to the audio file, any audio file compatible with ffmpeg will work
    input_audio_file: ""
    # Offset the audio by this many seconds. Useful when generating a scene only for a specific part in an audio file
    input_audio_offset: 0
    # Filters are only necessary for spectral type reactivity
    input_audio_filters: [ ]
    # input_audio_filters:
    # - variable_name: fLo # The name of the variable we will use in the animation function
    #   f_center: 80       # Central frequency the filter will respond to (80Hz)
    #   f_width: 20        # The filter will pass signals from 70Hz to 90Hz
    #   order: 6           # The filter slope will be 6*6=36dB per octave



# TODO: later: handle remaining tasks also:
# - upscaling?
# - video encoding?
# - interpolation?